@article{Lighthill1955,
abstract = {This paper uses the method of kinematic waves, developed in part I, but may be read independently. A functional relationship between flow and concentration for traffic on crowded arterial roads has been postulated for some time, and has experimental backing (section 2). From this a theory of the propagation of changes in traffic distribution along these roads may be deduced (section section 2, 3). The theory is applied (section 4) to the problem of estimating how a 'hump', or region of increased concentration, will move along a crowded main road. It is suggested that it will move slightly slower than the mean vehicle speed, and that vehicles passing through it will have to reduce speed rather suddenly (at a 'shock wave') on entering it, but can increase speed again only very gradually as they leave it. The hump gradually spreads out along the road, and the time scale of this process is estimated. The behaviour of such a hump on entering a bottleneck, which is too narrow to admit the increased flow, is studied (section 5), and methods are obtained for estimating the extent and duration of the resulting hold-up. The theory is applicable principally to traffic behaviour over a long stretch of road, but the paper concludes (section 6) with a discussion of its relevance to problems of flow near junctions, including a discussion of the starting flow at a controlled junction. In the introductory sections 1 and 2, we have included some elementary material on the quantitative study of traffic flow for the benefit of scientific readers unfamiliar with the subject.},
author = {Lighthill, M. J. and Whitham, G. B.},
doi = {10.1098/rspa.1955.0089},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/lighthill1955.pdf:pdf},
isbn = {00804630},
issn = {1364-5021},
journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
mendeley-groups = {Thesis},
number = {1178},
pages = {317--345},
title = {{On Kinematic Waves. II. A Theory of Traffic Flow on Long Crowded Roads}},
url = {http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.1955.0089},
volume = {229},
year = {1955}
}
@article{Richards1956,
abstract = {A simple theory of traffic flow is developed by replacing individual vehicles with a continuous 'fluid' density and applying an empirical relation between speed and density. Characteristic features of the resulting theory are a simple 'graph-shearing' process for following the development of traffic waves in time and the frequent appearance of shock waves. The effect of a traffic signal on traffic streams is studied and found to exhibit a threshold effect wherein the disturbances are minor for light traffic but suddenly build to large values when a critical density is exceeded.},
author = {Richards, Paul I.},
doi = {10.1287/opre.4.1.42},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/richards1956.pdf:pdf},
isbn = {0030-364X},
issn = {0030-364X},
journal = {Operations Research},
mendeley-groups = {Thesis},
number = {1},
pages = {42--51},
title = {{Shock Waves on the Highway}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.4.1.42},
volume = {4},
year = {1956}
}
@article{Ahmed1979,
abstract = {"Transportation Research Board [and] Commission on Sociotechnical Systems, National Research Council." "Reports of the 58th annual meeting of the Transportation Research Board"--Cataloging in Publication data [p. ii]. Papers sponsored by Committee on Urban System Operations, Committee on Parking and Terminals, and Committee on Freeway Operations. Analysis of freeway traffic time-series data by using Box-Jenkins techniques / Mohamed S. Ahmed, Allen R. Cook -- Automobile diversion : a strategy for reducing traffic in sensitive areas / Ronald H. Borowski -- Development and application of a freeway priority-lane model / Matthys P. Cilliers, Adolf D. May, Reed Cooper -- Safety considerations in the use of on-street parking / Jack B. Humphreys (and 3 others) -- Delay, time saved, and travel time information for freeway traffic management / R. Dale Hutchingson, Conrad L. Dudek -- Empirical analysis of the interdependence of parking restrictions and modal use / Curtis C. Lueck, Edward A. Beimborn -- Incident-detection algorithms. Part 1, Off-line evaluation / Moshe Levin, Gerianne M. Krause; Part 2, On-line evaluation / Moshe Levin, Gerianne Krause, James A. Budrick -- Development of a transport system management planning process in the Delaware Valley Region / Rasin K. Mufti, James J. Schwarzwalder -- FREFLO : a macroscopic simulation model of freeway traffic / Harold J. Payne -- Evaluation of the I-35 redesignation in San Antonio / William R. Stockton, Conrad L. Dudek, Donald B. Hatcher -- Improved air quality through transportation sysem management / John H. Suhrbier, Terry J. Atherton, Elizabeth A. Deakin -- Results of implementing low-cost freeway incident-management techniques / Gary L. Urbanek, Samuel C. Tignor, Gary C. Price -- High-occupancy vehicle considerations on an arterial corridor in Pensacola, Florida / Cecil O. Willis, Jr. -- Planning rail station parking : approach and application / L, K. Carpenter, E.M. Whitlock -- Development of freeway incident-detection algorithms by using pattern-recognition techniques / J. Tsai, E.R. Case.},
author = {Ahmed, Mohammed S and Cook, Allen R},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/722-001.pdf:pdf},
isbn = {0309029724},
issn = {03611981},
journal = {Transportation Research Record},
mendeley-groups = {Thesis},
number = {722},
pages = {116},
title = {{Analysis of freeway traffic time-series data by using Box-Jenkins techniques.}},
year = {1979}
}
@article{Davis1991,
author = {Davis, By Gary A and Member, Associate and Nihan, Nancy L},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/davis1991.pdf:pdf},
mendeley-groups = {Thesis},
number = {2},
pages = {178--188},
title = {{Nonparametric regression and short - term freeway traffic forecasting}},
volume = {117},
year = {1991}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called $\backslash$Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error through $\backslash$constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Hochreiter, Sepp and {Urgen Schmidhuber}, J},
doi = {10.1162/neco.1997.9.8.1735},
eprint = {1206.2944},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/2604.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Neural Computation},
mendeley-groups = {Thesis},
number = {8},
pages = {1735--1780},
pmid = {9377276},
title = {{Long Short-Term Memory}},
url = {http://www7.informatik.tu-muenchen.de/{~}hochreit{\%}5Cnhttp://www.idsia.ch/{~}juergen},
volume = {9},
year = {1997}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation$\backslash$nalgorithm constitute the best example of a successful gradient based$\backslash$nlearning technique. Given an appropriate network architecture,$\backslash$ngradient-based learning algorithms can be used to synthesize a complex$\backslash$ndecision surface that can classify high-dimensional patterns, such as$\backslash$nhandwritten characters, with minimal preprocessing. This paper reviews$\backslash$nvarious methods applied to handwritten character recognition and$\backslash$ncompares them on a standard handwritten digit recognition task.$\backslash$nConvolutional neural networks, which are specifically designed to deal$\backslash$nwith the variability of 2D shapes, are shown to outperform all other$\backslash$ntechniques. Real-life document recognition systems are composed of$\backslash$nmultiple modules including field extraction, segmentation recognition,$\backslash$nand language modeling. A new learning paradigm, called graph transformer$\backslash$nnetworks (GTN), allows such multimodule systems to be trained globally$\backslash$nusing gradient-based methods so as to minimize an overall performance$\backslash$nmeasure. Two systems for online handwriting recognition are described.$\backslash$nExperiments demonstrate the advantage of global training, and the$\backslash$nflexibility of graph transformer networks. A graph transformer network$\backslash$nfor reading a bank cheque is also described. It uses convolutional$\backslash$nneural network character recognizers combined with global training$\backslash$ntechniques to provide record accuracy on business and personal cheques.$\backslash$nIt is deployed commercially and reads several million cheques per day$\backslash$n},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/lecun-01a.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
mendeley-groups = {Thesis},
number = {11},
pages = {2278--2323},
pmid = {15823584},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Williams2003,
abstract = {This article presents the theoretical basis for modeling univariate traffic condition data streams as seasonal autoregressive integrated moving average processes. This foundation rests on the Wold decomposition theorem and on the assertion that a one-week lagged first seasonal difference applied to discrete interval traffic condition data will yield a weakly stationary transformation. Moreover, empirical results using actual intelligent transportation system data are presented and found to be consistent with the theoretical hypothesis. Conclusions are given on the implications of these assertions and findings relative to ongoing intelligent transportation systems research, deployment, and operations. ABSTRACT FROM AUTHOR},
author = {Williams, Billy M. and Hoel, Lester A.},
doi = {10.1061/(ASCE)0733-947X(2003)129:6(664)},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/10.1.1.460.4524.pdf:pdf},
isbn = {0-07-022045-X},
issn = {0733-947X},
journal = {Journal of Transportation Engineering},
mendeley-groups = {Thesis},
number = {6},
pages = {664--672},
title = {{Modeling and Forecasting Vehicular Traffic Flow as a Seasonal ARIMA Process: Theoretical Basis and Empirical Results}},
url = {http://ascelibrary.org/doi/10.1061/{\%}28ASCE{\%}290733-947X{\%}282003{\%}29129{\%}3A6{\%}28664{\%}29},
volume = {129},
year = {2003}
}
@article{Jun2008,
abstract = {Intelligent transportation system (ITS) is an effective measure to solve the problem of traffic jam. Accurate real-time predication of traffic flow is the key technology of ITS. In this paper a dynamic traffic flow forecasting model based on neural network is proposed. BP and RBF neural network are used to build the forecasting models. The data pre-handle method and the judgment criterion of the forecasting model are given. Simulation shows the traffic flow forecasting method is effective, and the RBF can be more fast and effective in forecasting the traffic flow by simulation analysis.},
author = {Jun, Ma Jun Ma and Ying, Meng Ying Meng},
doi = {10.1109/ICMLC.2003.1259684},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/jun2008.pdf:pdf},
isbn = {0780378652},
journal = {2008 Second International Symposium on Intelligent Information Technology Application},
keywords = {neural network,traffic flow forecasting},
mendeley-groups = {Thesis},
number = {973},
pages = {451--456},
title = {{Research of Traffic Flow Forecasting Based on Neural Network}},
url = {http://www.springerlink.com/index/jv68tg44u776l118.pdf},
volume = {2},
year = {2008}
}
@article{Castro-Neto2009,
abstract = {Most literature on short-term traffic flow forecasting focused mainly on normal, or non-incident, conditions and, hence, limited their applicability when traffic flow forecasting is most needed, i.e., incident and atypical conditions. Accurate prediction of short-term traffic flow under atypical conditions, such as vehicular crashes, inclement weather, work zone, and holidays, is crucial to effective and proactive traffic management systems in the context of intelligent transportation systems (ITS) and, more specifically, dynamic traffic assignment (DTA). To this end, this paper presents an application of a supervised statistical learning technique called Online Support Vector machine for Regression, or OL-SVR, for the prediction of short-term freeway traffic flow under both typical and atypical conditions. The OL-SVR model is compared with three well-known prediction models including Gaussian maximum likelihood (GML), Holt exponential smoothing, and artificial neural net models. The resultant performance comparisons suggest that GML, which relies heavily on the recurring characteristics of day-to-day traffic, performs slightly better than other models under typical traffic conditions, as demonstrated by previous studies. Yet OL-SVR is the best performer under non-recurring atypical traffic conditions. It appears that for deployed ITS systems that gear toward timely response to real-world atypical and incident situations, OL-SVR may be a better tool than GML. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
author = {Castro-Neto, Manoel and Jeong, Young Seon and Jeong, Myong Kee and Han, Lee D.},
doi = {10.1016/j.eswa.2008.07.069},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/castroneto2009.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Intelligent transportation systems (ITS),Online support vector machine (OL-SVM),Online support vector regression (OL-SVR),Short-term flow forecast,Traffic volume prediction},
mendeley-groups = {Thesis},
number = {3 PART 2},
pages = {6164--6173},
publisher = {Elsevier Ltd},
title = {{Online-SVR for short-term traffic flow prediction under typical and atypical traffic conditions}},
url = {http://dx.doi.org/10.1016/j.eswa.2008.07.069},
volume = {36},
year = {2009}
}
@article{Lippi2013,
author = {Lippi, Marco and Bertini, Marco and Frasconi, Paolo},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/lippi2013.pdf:pdf},
journal = {IEEE Transactions on Intelligent Transportation Systems},
mendeley-groups = {Thesis},
number = {2},
pages = {871--882},
title = {{Short-term traffic flow forecasting: An experimental comparison of time-series analysis and supervised learning}},
volume = {14},
year = {2013}
}
@article{Huang2014,
abstract = {Traffic flow prediction is a fundamental problem in transportation modeling and management. Many existing approaches fail to provide favorable results due to being: 1) shallow in architecture; 2) hand engineered in features; and 3) separate in learning. In this paper we propose a deep architecture that consists of two parts, i.e., a deep belief network (DBN) at the bottom and a multitask regression layer at the top. A DBN is employed here for unsupervised feature learning. It can learn effective features for traffic flow prediction in an unsupervised fashion, which has been examined and found to be effective for many areas such as image and audio classification. To the best of our knowledge, this is the first paper that applies the deep learning approach to transportation research. To incorporate multitask learning (MTL) in our deep architecture, a multitask regression layer is used above the DBN for supervised prediction. We further investigate homogeneous MTL and heterogeneous MTL for traffic flow prediction. To take full advantage of weight sharing in our deep architecture, we propose a grouping method based on the weights in the top layer to make MTL more effective. Experiments on transportation data sets show good performance of our deep architecture. Abundant experiments show that our approach achieved close to 5{\%} improvements over the state of the art. It is also presented that MTL can improve the generalization performance of shared tasks. These positive results demonstrate that deep learning and MTL are promising in transportation research.},
author = {Huang, Wenhao and Song, Guojie and Hong, Haikun and Xie, Kunqing},
doi = {10.1109/TITS.2014.2311123},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/huang2014.pdf:pdf},
isbn = {1524-9050 VO - 15},
issn = {15249050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
keywords = {Deep learning,Traffic flow prediction,multitask learning (MTL),task grouping},
mendeley-groups = {Thesis},
number = {5},
pages = {2191--2201},
title = {{Deep architecture for traffic flow prediction: Deep belief networks with multitask learning}},
volume = {15},
year = {2014}
}
@article{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
doi = {10.3115/v1/D14-1179},
eprint = {1406.1078},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/1406.1078.pdf:pdf},
isbn = {9781937284961},
issn = {09205691},
mendeley-groups = {Thesis},
pmid = {2079951},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078},
year = {2014}
}
@article{Shi2015,
abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
archivePrefix = {arXiv},
arxivId = {1506.04214},
author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
eprint = {1506.04214},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/1506.04214(2).pdf:pdf},
issn = {10495258},
mendeley-groups = {Thesis},
pages = {1--12},
title = {{Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting}},
url = {http://arxiv.org/abs/1506.04214},
year = {2015}
}
@article{Lv2014,
abstract = {Accurate and timely traffic flow information is important for the successful deployment of intelligent transportation systems. Over the last few years, traffic data have been exploding, and we have truly entered the era of big data for transportation. Existing traffic flow prediction methods mainly use shallow traffic prediction models and are still unsatisfying for many real-world applications. This situation inspires us to rethink the traffic flow prediction problem based on deep architecture models with big traffic data. In this paper, a novel deep-learning-based traffic flow prediction method is proposed, which considers the spatial and temporal correlations inherently. A stacked autoencoder model is used to learn generic traffic flow features, and it is trained in a greedy layerwise fashion. To the best of our knowledge, this is the first time that a deep architecture model is applied using autoencoders as building blocks to represent traffic flow features for prediction. Moreover, experiments demonstrate that the proposed method for traffic flow prediction has superior performance.},
author = {Lv, Yisheng and Duan, Yanjie and Kang, Wenwen and Li, Zhengxi and Wang, Fei Yue},
doi = {10.1109/TITS.2014.2345663},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/lv2014(2).pdf:pdf},
isbn = {1524-9050},
issn = {15249050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
keywords = {Deep learning,stacked autoencoders (SAEs),traffic flow prediction},
mendeley-groups = {Thesis},
number = {2},
pages = {865--873},
title = {{Traffic Flow Prediction with Big Data: A Deep Learning Approach}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6894591},
volume = {16},
year = {2015}
}
@article{Bengio2015,
abstract = {Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used successfully in our winning entry to the MSCOCO image captioning challenge, 2015.},
archivePrefix = {arXiv},
arxivId = {1506.03099},
author = {Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
doi = {10.1201/9781420049176},
eprint = {1506.03099},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/1506.03099.pdf:pdf},
isbn = {9789537619084},
issn = {0302-9743},
mendeley-groups = {Thesis},
month = {jun},
pages = {1--9},
pmid = {21803542},
title = {{Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1506.03099},
year = {2015}
}
@article{Defferrard2016,
abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
archivePrefix = {arXiv},
arxivId = {1606.09375},
author = {Defferrard, Micha{\"{e}}l and Bresson, Xavier and Vandergheynst, Pierre},
eprint = {1606.09375},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/GraphConvolution.pdf:pdf},
isbn = {978-1-5108-3881-9},
issn = {10495258},
mendeley-groups = {Thesis},
number = {Nips},
title = {{Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering}},
url = {http://arxiv.org/abs/1606.09375},
year = {2016}
}
@article{Seo2016,
abstract = {This paper introduces Graph Convolutional Recurrent Network (GCRN), a deep learning model able to predict structured sequences of data. Precisely, GCRN is a generalization of classical recurrent neural networks (RNN) to data structured by an arbitrary graph. Such structured sequences can represent series of frames in videos, spatio-temporal measurements on a network of sensors, or random walks on a vocabulary graph for natural language modeling. The proposed model combines convolutional neural networks (CNN) on graphs to identify spatial structures and RNN to find dynamic patterns. We study two possible architectures of GCRN, and apply the models to two practical problems: predicting moving MNIST data, and modeling natural language with the Penn Treebank dataset. Experiments show that exploiting simultaneously graph spatial and dynamic information about data can improve both precision and learning speed.},
archivePrefix = {arXiv},
arxivId = {1612.07659},
author = {Seo, Youngjoo and Defferrard, Micha{\"{e}}l and Vandergheynst, Pierre and Bresson, Xavier},
doi = {arXiv:1602.00203},
eprint = {1612.07659},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/one{\_}more{\_}graphconv.pdf:pdf},
issn = {2169-3536},
mendeley-groups = {Thesis},
number = {2013},
pages = {1--10},
title = {{Structured Sequence Modeling with Graph Convolutional Recurrent Networks}},
url = {http://arxiv.org/abs/1612.07659},
year = {2016}
}
@article{Neil2016,
abstract = {Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for extracting patterns from temporal sequences. However, current RNN models are ill-suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons. Such data can occur, for example, when the input comes from novel event-driven artificial sensors that generate sparse, asynchronous streams of events or from multiple conventional sensors with different update intervals. In this work, we introduce the Phased LSTM model, which extends the LSTM unit by adding a new time gate. This gate is controlled by a parametrized oscillation with a frequency range that produces updates of the memory cell only during a small percentage of the cycle. Even with the sparse updates imposed by the oscillation, the Phased LSTM network achieves faster convergence than regular LSTMs on tasks which require learning of long sequences. The model naturally integrates inputs from sensors of arbitrary sampling rates, thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information. It also greatly improves the performance of LSTMs in standard RNN applications, and does so with an order-of-magnitude fewer computes at runtime.},
archivePrefix = {arXiv},
arxivId = {1610.09513},
author = {Neil, Daniel and Pfeiffer, Michael and Liu, Shih-Chii},
eprint = {1610.09513},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/1610.09513(1).pdf:pdf},
issn = {10495258},
mendeley-groups = {Thesis},
number = {Nips},
title = {{Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences}},
url = {http://arxiv.org/abs/1610.09513},
year = {2016}
}
@article{Chen2016,
abstract = {With the rapid development of urbanization and public transportation system, the number of traffic accidents have signifi- cantly increased globally over the past decades and become a big problem for human society. Facing these possible and unexpected traffic accidents, understanding what causes traffic accident and early alarms for some possible ones will play a critical role on planning effective traffic management. How- ever, due to the lack of supported sensing data, research is very limited on the field of updating traffic accident risk in real-time. Therefore, in this paper, we collect big and heterogeneous data (7 months traffic accident data and 1.6 million users' GPS records) to understand how human mobility will affect traffic accident risk. By mining these data, we develop a deep model of Stack denoise Autoencoder to learn hierar- chical feature representation of human mobility. And these features are used for efficient prediction of traffic accident risk level. Once the model has been trained, our model can simulate corresponding traffic accident risk map with given real-time input of human mobility. The experimental results demonstrate the efficiency of our model and suggest that traffic accident risk can be significantly more predictable through human mobility.},
author = {Chen, Quanjun and Song, Xuan and Yamada, Harutoshi and Shibasaki, Ryosuke},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/AAAI2016.pdf:pdf},
isbn = {9781577357605},
journal = {Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-16)},
keywords = {Technical Papers: Computational Sustainability and},
mendeley-groups = {Thesis},
pages = {338--344},
title = {{Learning Deep Representation from Big and Heterogeneous Data for Traffic Accident Inference}},
year = {2016}
}
@inproceedings{kipf2017semi,
  title={Semi-Supervised Classification with Graph Convolutional Networks},
  author={Kipf, Thomas N. and Welling, Max},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017}
}
@article{Li2017,
abstract = {Spatiotemporal forecasting has various applications in neuroscience, climate and transportation domain. Traffic forecasting is one canonical example of such learning task. The task is challenging due to (1) complex spatial dependency on road networks, (2) non-linear temporal dynamics with changing road conditions and (3) inherent difficulty of long-term forecasting. To address these challenges, we propose to model the traffic flow as a diffusion process on a directed graph and introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep learning framework for traffic forecasting that incorporates both spatial and temporal dependency in the traffic flow. Specifically, DCRNN captures the spatial dependency using bidirectional random walks on the graph, and the temporal dependency using the encoder-decoder architecture with scheduled sampling. We evaluate the framework on two real-world large scale road network traffic datasets and observe consistent improvement of 12{\%} - 15{\%} over state-of-the-art baselines.},
archivePrefix = {arXiv},
arxivId = {1707.01926},
author = {Li, Yaguang and Yu, Rose and Shahabi, Cyrus and Liu, Yan},
doi = {10.15662/IJAREEIE.2015.0501067},
eprint = {1707.01926},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/GraphConv{\_}Rnn.pdf:pdf},
isbn = {9781461268758},
issn = {1045-9227},
mendeley-groups = {Thesis},
pages = {1--12},
pmid = {17756722},
title = {{Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting}},
url = {http://arxiv.org/abs/1707.01926},
year = {2017}
}
@article{Yu2017,
abstract = {Timely accurate traffic forecast is crucial for urban traffic control and guidance. Due to the high nonlinearity and complexity of traffic flow, traditional methods cannot satisfy the requirements of mid-and-long term prediction tasks and often neglect spatial and temporal dependencies. In this paper, we propose a novel deep learning framework, Spatio-Temporal Graph Convolutional Networks (STGCN), to tackle the time series prediction problem in traffic domain. Instead of applying regular convolutional and recurrent units, we formulate the problem on graphs and build the model with complete convolutional structures, which enable much faster training speed with fewer parameters. Experiments show that our STGCN model effectively captures comprehensive spatio-temporal correlations through modeling multi-scale traffic networks and consistently outperforms state-of-the-art baselines on various real-world traffic datasets.},
archivePrefix = {arXiv},
arxivId = {1709.04875},
author = {Yu, Bing and Yin, Haoteng and Zhu, Zhanxing},
eprint = {1709.04875},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/TimeSeriesConv.pdf:pdf},
mendeley-groups = {Thesis},
title = {{Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting}},
url = {http://arxiv.org/abs/1709.04875},
year = {2017}
}
@article{Kim2017,
abstract = {In this paper, a novel architecture for a deep recurrent neural network, residual LSTM is introduced. A plain LSTM has an internal memory cell that can learn long term dependencies of sequential data. It also provides a temporal shortcut path to avoid vanishing or exploding gradients in the temporal domain. The residual LSTM provides an additional spatial shortcut path from lower layers for efficient training of deep networks with multiple LSTM layers. Compared with the previous work, highway LSTM, residual LSTM separates a spatial shortcut path with temporal one by using output layers, which can help to avoid a conflict between spatial and temporal-domain gradient flows. Furthermore, residual LSTM reuses the output projection matrix and the output gate of LSTM to control the spatial information flow instead of additional gate networks, which effectively reduces more than 10{\%} of network parameters. An experiment for distant speech recognition on the AMI SDM corpus shows that 10-layer plain and highway LSTM networks presented 13.7{\%} and 6.2{\%} increase in WER over 3-layer aselines, respectively. On the contrary, 10-layer residual LSTM networks provided the lowest WER 41.0{\%}, which corresponds to 3.3{\%} and 2.8{\%} WER reduction over plain and highway LSTM networks, respectively.},
archivePrefix = {arXiv},
arxivId = {1701.03360},
author = {Kim, Jaeyoung and El-Khamy, Mostafa and Lee, Jungwon},
doi = {10.21437/Interspeech.2017-477},
eprint = {1701.03360},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/1701.03360.pdf:pdf},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {ASR,CNN,GMM,LSTM,RNN},
mendeley-groups = {Thesis},
pages = {1591--1595},
title = {{Residual LSTM: Design of a deep recurrent architecture for distant speech recognition}},
url = {http://arxiv.org/abs/1701.03360},
volume = {2017-Augus},
year = {2017}
}
@article{Greff2017,
abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs ({\$}\backslashapprox 15{\$} years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
archivePrefix = {arXiv},
arxivId = {1503.04069},
author = {Greff, Klaus and Srivastava, Rupesh K. and Koutnik, Jan and Steunebrink, Bas R. and Schmidhuber, Jurgen},
doi = {10.1109/TNNLS.2016.2582924},
eprint = {1503.04069},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/1503.04069.pdf:pdf},
isbn = {9788578110796},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Functional ANalysis Of VAriance (fANOVA),long short-term memory (LSTM),random search,recurrent neural networks,sequence learning},
mendeley-groups = {Thesis},
number = {10},
pages = {2222--2232},
pmid = {25246403},
title = {{LSTM: A Search Space Odyssey}},
volume = {28},
year = {2017}
}
@article{Li2018,
abstract = {Recurrent neural networks (RNNs) have been widely used for processing sequential data. However, RNNs are commonly difficult to train due to the well-known gradient vanishing and exploding problems and hard to learn long-term patterns. Long short-term memory (LSTM) and gated recurrent unit (GRU) were developed to address these problems, but the use of hyperbolic tangent and the sigmoid action functions results in gradient decay over layers. Consequently, construction of an efficiently trainable deep network is challenging. In addition, all the neurons in an RNN layer are entangled together and their behaviour is hard to interpret. To address these problems, a new type of RNN, referred to as independently recurrent neural network (IndRNN), is proposed in this paper, where neurons in the same layer are independent of each other and they are connected across layers. We have shown that an IndRNN can be easily regulated to prevent the gradient exploding and vanishing problems while allowing the network to learn long-term dependencies. Moreover, an IndRNN can work with non-saturated activation functions such as relu (rectified linear unit) and be still trained robustly. Multiple IndRNNs can be stacked to construct a network that is deeper than the existing RNNs. Experimental results have shown that the proposed IndRNN is able to process very long sequences (over 5000 time steps), can be used to construct very deep networks (21 layers used in the experiment) and still be trained robustly. Better performances have been achieved on various tasks by using IndRNNs compared with the traditional RNN and LSTM.},
archivePrefix = {arXiv},
arxivId = {1803.04831},
author = {Li, Shuai and Li, Wanqing and Cook, Chris and Zhu, Ce and Gao, Yanbo},
eprint = {1803.04831},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/1803.04831.pdf:pdf},
mendeley-groups = {Thesis},
number = {1},
title = {{Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN}},
url = {http://arxiv.org/abs/1803.04831},
year = {2018}
}
@article{Kingma2015,
archivePrefix = {arXiv},
arxivId = {1412.6980v9},
author = {Kingma, Diederik P and Ba, Jimmy Lei},
eprint = {1412.6980v9},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/1412.6980.pdf:pdf},
pages = {1--15},
title = {{Adam: A Method for Stochastic Optimization}},
year = {2015}
}
@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}
@misc{pems,
title = {{C}altrans {P}erformance {M}easurement -- {S}tate of {C}alifornia},
url = {http://pems.dot.ca.gov/},
note = {http://pems.dot.ca.gov/}
}
@article{Chen2001,
author = {Chen, Chao and Petty, Karl and Skabardonis, Alexander and Varaiya, Pravin and Jia, Zhanfeng},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/chen2001.pdf:pdf},
number = {01},
pages = {96--102},
title = {{Mining Loop Detector Data}}
year = {2001}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
doi = {10.1007/s10107-014-0839-0},
eprint = {1409.3215},
file = {:C$\backslash$:/Users/nprok/Documents/Thesis/Citess/1409.3215.pdf:pdf},
isbn = {1409.3215},
issn = {09205691},
pages = {1--9},
pmid = {2079951},
title = {{Sequence to Sequence Learning with Neural Networks}},
url = {http://arxiv.org/abs/1409.3215},
year = {2014}
}
